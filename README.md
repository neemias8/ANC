# **Abstractive Narrative Consolidation (ANC)**

## **Comparing Extractive and Abstractive Methods for Gospel Consolidation**

This repository explores **Narrative Consolidation** by comparing extractive (TAEG) and abstractive (PRIMERA) approaches for unifying multiple gospel accounts of the Passion Week. The project investigates whether state-of-the-art multi-document summarization models can achieve comparable performance to purpose-built extractive methods while maintaining chronological integrity and factual accuracy.

> üìã **TL;DR**: **PRIMERA Event-by-Event achieves near-perfect results** (œÑ=0.976, 99.4% coverage) using event-based prompting, making it a viable abstractive alternative to TAEG. Standard MDS approaches fail at narrative consolidation. See full findings below.

### **Background: The Narrative Consolidation Task**

**Narrative Consolidation** differs fundamentally from traditional Multi-Document Summarization (MDS):

| Aspect | Traditional MDS | Narrative Consolidation |
|--------|----------------|------------------------|
| **Goal** | Conciseness | Completeness & Coherence |
| **Length** | Shorter than sources | As long as needed |
| **Temporal Order** | Not prioritized | **Critical requirement** |
| **Information** | Select salient points | Integrate all complementary details |

**Original TAEG Method** (Extractive):
- Builds Temporal Alignment Event Graph
- Selects best sentence per event chronologically
- Achieves Kendall's Tau = 1.000 (perfect order)
- ROUGE-L = 0.947 (comprehensive coverage)

**This Work** investigates whether **PRIMERA** (abstractive) can match or exceed TAEG while improving text fluency.

---

## **üî¨ Key Findings**

### **1. PRIMERA Event-by-Event Approach Succeeds**
- **Kendall's Tau: 0.9758** (97.6% chronological accuracy)
- **Coverage: 162/163 events** (99.4%; only event #53 missing)
- **Method**: Event-based prompting with numbered structure
- **Result**: Abstractive model achieves near-perfect temporal ordering

### **2. Event ID-Based Kendall's Tau Metric**
Traditional Kendall's Tau compares sentence-level ordering, which penalizes abstractive methods for paraphrasing identical content. We developed an **event ID-based metric** that:
- Extracts numbered events (e.g., "42 Jesus enters Jerusalem")
- Compares only the relative ordering of common events
- Ignores paraphrasing/rephrasing differences
- Reports coverage (events found / total events)

**Implementation**: Regex pattern `\b(\d+)\s+"?([A-Z])` extracts event IDs from both reference and hypothesis summaries, then calculates rank correlation on common events only.

### **3. Discovery: PRIMERA's Event-Based Capability**
Through trial-and-error, we discovered PRIMERA can maintain event structure when prompted to:
1. Number each event sequentially
2. Use one sentence per event
3. Maintain chronological order

This approach yields dramatically better results than standard MDS prompting (œÑ=0.976 vs œÑ=0.51).

### **4. Comparative Model Performance**

| Model | Kendall's Tau | Event Coverage | Type | Notes |
|-------|--------------|----------------|------|-------|
| **TAEG (LexRank-TA)** | **1.0000** | 163/163 (100%) | Extractive | Perfect baseline |
| **PRIMERA Event-by-Event** | **0.9758** | 162/163 (99.4%) | Abstractive | Near-perfect |
| **PRIMERA-MDS** | 0.5104 | ~60% | Abstractive | Standard mode |
| **BART-Large-CNN** | 0.5802 | ~65% | Abstractive | Single-doc trained |
| **PEGASUS-Large** | 0.1110 | ~30% | Abstractive | Poor chronology |
| **PEGASUS-XSUM** | 0.0846 | ~25% | Abstractive | Extreme compression |

**Key Insights:**
- Event-based prompting is critical for chronological accuracy
- Standard MDS models struggle with temporal ordering across documents
- Only 1 event truly missing from PRIMERA output (event #53)
- TAEG remains the gold standard but PRIMERA Event-by-Event is competitive

### **5. Hallucination Analysis**

**Hallucination** refers to factually incorrect information generated by abstractive models‚Äîcontent not present in the source documents.

| Model | Hallucination Level | Factual Accuracy | Example Issues |
|-------|-------------------|------------------|----------------|
| **TAEG** | ‚úÖ **None** | 100% | Extractive‚Äîcopies sentences verbatim |
| **PRIMERA Event-by-Event** | ‚úÖ **Minimal** | ~99% | Minor paraphrasing; **1 neural collapse** (Event #83) |
| **PRIMERA-MDS** | ‚ö†Ô∏è **Severe** | ~40% | **Fabricated narrative**, nonsensical sequences |
| **BART-Large-CNN** | ‚ö†Ô∏è **Moderate** | ~75% | Invented dialogue, compressed events |
| **PEGASUS-Large** | ‚ö†Ô∏è **Moderate** | ~70% | Generic summaries, missing specifics |
| **PEGASUS-XSUM** | ‚ö†Ô∏è **Severe** | ~50% | Extreme compression loses factual grounding |

**Detailed Findings:**

**TAEG (Extractive):**
- Zero hallucinations‚Äîall text directly extracted from source gospels
- Maintains perfect factual fidelity by design

**PRIMERA Event-by-Event:**
- **Minimal hallucinations** (~1% error rate)
- Maintains factual accuracy through numbered event structure
- Minor issues: occasional paraphrasing imprecision (e.g., "Jesus answered them" vs. "Jesus replied")
- No fabricated events or invented content
- Event-based prompting enforces grounding in source material
- **Critical Exception: Event #83 shows neural degeneracy** - model "collapsed" mid-generation, producing completely incoherent output mixing gospel content with modern references (COVID-19, school shootings, technology instructions). This represents ~0.6% of all events and appears to be a one-time generation failure rather than systematic hallucination.

**PRIMERA Standard MDS:**
- **CRITICAL: Severe hallucinations detected**
- Generates incoherent, fabricated narrative segments
- Example hallucinations:
  - *"They are the masters of the land and the people are the serpents of the sea"* (completely fabricated)
  - *"And the serpent is the master of the house of God"* (theological nonsense)
  - Mixed unrelated events into incomprehensible sequences
  - Created dialogue that never occurred
- **Root cause:** Standard MDS mode prioritizes conciseness over accuracy, leading to aggressive compression and creative rewriting

**BART-Large-CNN:**
- Moderate hallucinations due to single-document training
- Tends to invent connective phrases and simplified dialogue
- Loses factual precision in multi-document fusion

**PEGASUS Models:**
- Both variants show significant hallucinations
- Extreme summarization (XSUM) produces generic, factually untethered summaries
- Trained on news articles‚Äîstruggles with narrative consolidation requirements

**Critical Takeaway:**
- **Event-based structuring prevents hallucinations** in abstractive models
- PRIMERA Event-by-Event achieves 99% factual accuracy through:
  1. Numbered event constraints
  2. One-event-per-generation focus
  3. Explicit temporal ordering
- Standard MDS prompting produces dangerous hallucinations for consolidation tasks
- **Neural degeneracy risk:** Even constrained models can experience rare generation failures (Event #83: model "collapse" with complete loss of coherence, mixing modern content like "COVID-19", "AR-15", "HTML5 video" with biblical narrative)

---

## **Three Methods Compared**
## **Three Methods Compared**

| Method | Type | Chronological Order | Coverage | Factual Accuracy | Best For |
|--------|------|-------------------|----------|------------------|----------|
| **TAEG** | Extractive | ‚úÖ Perfect (œÑ=1.0) | ‚úÖ Complete (100%) | ‚úÖ 100% | Literal preservation |
| **PRIMERA Event-by-Event** | Abstractive | ‚úÖ Near-perfect (œÑ=0.976) | ‚úÖ 99.4% (162/163) | ‚úÖ 99% | Fluency + accuracy |
| **PRIMERA-MDS** | Abstractive | ‚ö†Ô∏è Poor (œÑ=0.51) | ‚ùå Concise only | ‚ùå ~40% | ‚ùå Not recommended |

**Key Discovery**: PRIMERA Event-by-Event achieves **near-perfect chronological order** (œÑ=0.976) by processing events individually with numbered IDs. Only **1 event missing** (event #53) out of 163 total events.

**Conclusion**: 
- **TAEG** remains superior for **perfect order** and **literal preservation**
- **PRIMERA Event-by-Event** is viable alternative with **97.6% order accuracy** and better text fluency
- Standard PRIMERA-MDS unsuitable for consolidation tasks (generates concise summaries)

---

## **Dataset: Gospel Consolidation Language Resource**

- **Four Gospels (NIV 2011)**: Matthew, Mark, Luke, John (Passion Week focus)
- **169 Canonical Events**: Palm Sunday to Resurrection
- **Golden Sample**: Expert-crafted reference (~79,000 characters)
- **Format**: XML with `book:chapter:verse` alignment (language-agnostic)

---

## **Evaluation Metrics**

All methods are evaluated against the **Golden Sample** using identical metrics:

### **Content Metrics**
- **ROUGE-1 / ROUGE-2**: Unigram and bigram overlap with reference
- **ROUGE-L**: Longest common subsequence (rewards chronological ordering)
- **METEOR**: Word alignment with synonymy and stemming support
- **BERTScore**: Semantic similarity using contextual embeddings

### **Temporal Ordering Metric**
- **Kendall's Tau (œÑ)**: Correlation of event ordering (-1 to +1)
  - œÑ = 1.0 ‚Üí Perfect chronological order
  - œÑ = 0.0 ‚Üí Random ordering
  - œÑ < 0 ‚Üí Inversions present

### **Expected Performance**

| Metric | TAEG (Baseline) | PRIMERA Event-by-Event | PRIMERA-MDS |
|--------|----------------|----------------------|-------------|
| **Kendall's Tau** | 1.000 | **0.976** | ~0.51 |
| **Event Coverage** | 163/163 (100%) | **162/163 (99.4%)** | ~60% |
| **ROUGE-1 F1** | 0.958 | ~0.90-0.95 | ~0.7-0.8 |
| **ROUGE-2 F1** | 0.938 | ~0.85-0.92 | ~0.6-0.7 |
| **ROUGE-L F1** | 0.947 | ~0.90-0.95 | ~0.5-0.6 |
| **BERTScore F1** | 0.995 | ~0.95-0.98 | ~0.85-0.90 |
| **METEOR** | 0.639 | ~0.60-0.70 | ~0.4-0.5 |
| **Text Fluency** | Good | ‚úÖ Excellent | ‚úÖ Excellent |

**Key Findings:**
1. PRIMERA Event-by-Event achieves near-perfect temporal order (œÑ = 0.976)
2. Only 1 event (#53) missing from output; all others present and correctly ordered
3. Event-based prompting is critical for chronological accuracy
4. Standard PRIMERA-MDS shows poor temporal ordering (œÑ = 0.51)

---

## **Project Structure**

```
‚îú‚îÄ‚îÄ README.md                          # This file
‚îú‚îÄ‚îÄ DESCOBERTAS_PROYECTO.md            # üî¨ Detailed findings and recommendations
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ EnglishNIVMatthew40_PW.xml    # Gospel texts
‚îÇ   ‚îú‚îÄ‚îÄ EnglishNIVMark41_PW.xml
‚îÇ   ‚îú‚îÄ‚îÄ EnglishNIVLuke42_PW.xml
‚îÇ   ‚îú‚îÄ‚îÄ EnglishNIVJohn43_PW.xml
‚îÇ   ‚îú‚îÄ‚îÄ ChronologyOfTheFourGospels_PW.xml  # 169 event mappings
‚îÇ   ‚îî‚îÄ‚îÄ Golden_Sample.txt              # Reference consolidation
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py                 # XML parsing
‚îÇ   ‚îú‚îÄ‚îÄ preprocess.py                  # Event-based segmentation
‚îÇ   ‚îú‚îÄ‚îÄ summarize_baseline.py          # PRIMERA standard MDS
‚îÇ   ‚îú‚îÄ‚îÄ consolidate_abstractive.py     # PRIMERA event-based consolidation
‚îÇ   ‚îú‚îÄ‚îÄ evaluator.py                   # Metrics (ROUGE, BERTScore, Kendall's Tau)
‚îÇ   ‚îú‚îÄ‚îÄ utils.py                       # PRIMERA utilities
‚îÇ   ‚îú‚îÄ‚îÄ summarizer.py                  # LexRank methods
‚îÇ   ‚îî‚îÄ‚îÄ main.py                        # TAEG pipeline
‚îú‚îÄ‚îÄ outputs/
‚îÇ   ‚îú‚îÄ‚îÄ primera_mds_output.txt
‚îÇ   ‚îú‚îÄ‚îÄ primera_consolidation.txt
‚îÇ   ‚îú‚îÄ‚îÄ taeg_summary_lexrank-ta.txt
‚îÇ   ‚îú‚îÄ‚îÄ comparison_report.txt          # üìä Three-way comparison
‚îÇ   ‚îî‚îÄ‚îÄ evaluation/                    # JSON metrics
‚îú‚îÄ‚îÄ compare_all_methods.py             # üöÄ Main comparison script
‚îî‚îÄ‚îÄ requirements.txt
```

---

## **üöÄ Quick Start**

### **Installation**

```bash
# Clone and setup
git clone https://github.com/neemias8/ANC.git
cd ANC
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# Download NLTK data
python -c "import nltk; nltk.download('punkt'); nltk.download('wordnet')"
```

### **Run Comparison**

```bash
# Compare all three methods with 10 events
python compare_all_methods.py --max-events 10 --device cpu

# Full comparison (169 events, ~5 hours on CPU)
python compare_all_methods.py --device cpu
```

### **Run Individual Methods**

```bash
# TAEG (extractive)
python src/main.py --method lexrank-ta --output-dir outputs

# PRIMERA-MDS (abstractive concise)
python src/summarize_baseline.py --max-length 512

# PRIMERA-Consolidation (abstractive comprehensive)
python src/consolidate_abstractive.py --max-length-per-event 256
```

**Important**: Use `--max-events 10` for quick testing. Full runs take hours on CPU.

---

## **Evaluation Metrics**

- **Kendall's Tau (œÑ)**: Chronological order correlation (-1 to +1)
- **ROUGE-1/2/L**: Lexical overlap with reference
- **BERTScore**: Semantic similarity using embeddings
- **METEOR**: Word alignment with synonymy

See [`outputs/comparison_report.txt`](outputs/comparison_report.txt) for detailed results.

---

## **Dependencies**

**Core**:
- `transformers==4.57.1` - PRIMERA model
- `torch>=2.6.0` - Deep learning framework  
- `lexrank==0.1.0` - TAEG method
- `rouge-score`, `bert-score` - Evaluation metrics
- `beautifulsoup4`, `lxml` - XML parsing

**Full list**: See `requirements.txt`

**Hardware**:
- Minimum: 16GB RAM, CPU
- Recommended: GPU with 8GB+ VRAM
- CPU runtime: ~5 hours for full 169 events

---

## **Citation**

```bibtex
@misc{anc2025,
  title={Abstractive Narrative Consolidation: Comparing Extractive and Abstractive Methods},
  author={[Your Name]},
  year={2025},
  note={Extends TAEG framework with PRIMERA experiments}
}
```

---

## **License**

MIT License - See `LICENSE` for details.

---

## **Key Takeaways**

1. ‚úÖ **PRIMERA Event-by-Event achieves near-perfect results** (œÑ=0.976, 99.4% coverage, 99% factual accuracy)
2. ‚úÖ **TAEG remains gold standard** for perfect chronological order (œÑ=1.0) and zero hallucinations
3. üî¨ **Event-based prompting is critical** - prevents hallucinations and maintains temporal order
4. ‚ö†Ô∏è **PRIMERA Standard MDS produces severe hallucinations** - fabricated narrative, theological nonsense
5. üìä **Event ID-based Kendall's Tau** reveals true chronological accuracy beyond sentence matching
6. ‚ö†Ô∏è **Only 1 event missing** from PRIMERA output (event #53 out of 163)
7. ‚ùå **General-purpose models fail** - BART, PEGASUS show poor temporal ordering (œÑ<0.6) and moderate hallucinations
8. üéØ **Factual accuracy vs fluency tradeoff** - Event-based structure prevents hallucinations while maintaining readability

> üìñ **Read [`DESCOBERTAS_PROYECTO.md`](DESCOBERTAS_PROYECTO.md) for detailed technical findings, implementation notes, and recommendations for using PRIMERA in narrative consolidation projects.**

---

**Questions?** Open an issue on GitHub.
