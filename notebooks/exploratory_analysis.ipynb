{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f8d0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "````xml\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "# Exploratory Analysis: TAEG vs PRIMERA Methods\n",
    "\n",
    "This notebook provides qualitative analysis and visualization comparing the three approaches for narrative consolidation:\n",
    "\n",
    "1. **TAEG** - Extractive with temporal graph\n",
    "2. **PRIMERA-MDS** - Abstractive standard summarization\n",
    "3. **PRIMERA-Consolidation** - Abstractive with event-based segmentation\n",
    "\n",
    "## Setup\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Import required libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Setup complete\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## Load Outputs and Evaluation Results\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Define paths\n",
    "OUTPUT_DIR = Path(\"../outputs\")\n",
    "EVAL_DIR = OUTPUT_DIR / \"evaluation\"\n",
    "\n",
    "# Load outputs\n",
    "def load_output(filename):\n",
    "    \"\"\"Load output text file.\"\"\"\n",
    "    filepath = OUTPUT_DIR / filename\n",
    "    if filepath.exists():\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    return None\n",
    "\n",
    "# Load evaluation results\n",
    "def load_evaluation(filename):\n",
    "    \"\"\"Load evaluation JSON file.\"\"\"\n",
    "    filepath = EVAL_DIR / filename\n",
    "    if filepath.exists():\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "# Load all methods\n",
    "methods = {\n",
    "    \"TAEG\": {\n",
    "        \"output\": load_output(\"taeg_summary_lexrank-ta.txt\"),\n",
    "        \"eval\": load_evaluation(\"taeg_results.json\")\n",
    "    },\n",
    "    \"PRIMERA-MDS\": {\n",
    "        \"output\": load_output(\"primera_mds_output.txt\"),\n",
    "        \"eval\": load_evaluation(\"primera_mds_results.json\")\n",
    "    },\n",
    "    \"PRIMERA-Consolidation\": {\n",
    "        \"output\": load_output(\"primera_consolidation.txt\"),\n",
    "        \"eval\": load_evaluation(\"primera_consolidation_results.json\")\n",
    "    }\n",
    "}\n",
    "\n",
    "# Check what's available\n",
    "for method, data in methods.items():\n",
    "    output_status = \"‚úÖ\" if data[\"output\"] else \"‚ùå\"\n",
    "    eval_status = \"‚úÖ\" if data[\"eval\"] else \"‚ùå\"\n",
    "    print(f\"{method:25} - Output: {output_status}  Evaluation: {eval_status}\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## Metric Comparison Visualization\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Extract metrics into DataFrame\n",
    "def extract_metrics(methods_dict):\n",
    "    \"\"\"Extract metrics from evaluation results.\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for method, content in methods_dict.items():\n",
    "        if content[\"eval\"] is None:\n",
    "            continue\n",
    "        \n",
    "        eval_data = content[\"eval\"]\n",
    "        \n",
    "        row = {\n",
    "            \"Method\": method,\n",
    "            \"Kendall's Tau\": eval_data.get(\"kendall_tau\", 0),\n",
    "            \"ROUGE-1 F1\": eval_data.get(\"rouge\", {}).get(\"rouge1\", {}).get(\"f1\", 0),\n",
    "            \"ROUGE-2 F1\": eval_data.get(\"rouge\", {}).get(\"rouge2\", {}).get(\"f1\", 0),\n",
    "            \"ROUGE-L F1\": eval_data.get(\"rouge\", {}).get(\"rougeL\", {}).get(\"f1\", 0),\n",
    "            \"BERTScore F1\": eval_data.get(\"bertscore\", {}).get(\"f1\", 0),\n",
    "            \"METEOR\": eval_data.get(\"meteor\", 0),\n",
    "        }\n",
    "        \n",
    "        data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Create DataFrame\n",
    "df_metrics = extract_metrics(methods)\n",
    "\n",
    "if not df_metrics.empty:\n",
    "    display(df_metrics.set_index(\"Method\"))\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    metrics_to_plot = [\"Kendall's Tau\", \"ROUGE-1 F1\", \"ROUGE-2 F1\", \n",
    "                       \"ROUGE-L F1\", \"BERTScore F1\", \"METEOR\"]\n",
    "    \n",
    "    for idx, metric in enumerate(metrics_to_plot):\n",
    "        ax = axes[idx]\n",
    "        df_metrics.plot(x=\"Method\", y=metric, kind=\"bar\", ax=ax, legend=False)\n",
    "        ax.set_title(metric, fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel(\"Score\")\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylim(0, 1.0)\n",
    "        ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.3, label='0.5 threshold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Rotate x labels\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / \"metric_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìä Metrics visualization saved to outputs/metric_comparison.png\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No evaluation data available to plot\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## Output Length Comparison\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Compare output lengths\n",
    "length_data = []\n",
    "\n",
    "for method, content in methods.items():\n",
    "    if content[\"output\"]:\n",
    "        length_data.append({\n",
    "            \"Method\": method,\n",
    "            \"Characters\": len(content[\"output\"]),\n",
    "            \"Words\": len(content[\"output\"].split()),\n",
    "            \"Sentences\": content[\"output\"].count('.') + content[\"output\"].count('!') + content[\"output\"].count('?')\n",
    "        })\n",
    "\n",
    "df_lengths = pd.DataFrame(length_data)\n",
    "\n",
    "if not df_lengths.empty:\n",
    "    display(df_lengths.set_index(\"Method\"))\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    df_lengths.plot(x=\"Method\", y=[\"Characters\", \"Words\", \"Sentences\"], \n",
    "                    kind=\"bar\", ax=ax)\n",
    "    ax.set_title(\"Output Length Comparison\", fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.legend(title=\"Unit\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / \"length_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìä Length visualization saved to outputs/length_comparison.png\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## Sample Text Comparison\n",
    "\n",
    "Let's examine the first few sentences from each method to assess fluency and style.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "def display_sample(method_name, text, num_sentences=5):\n",
    "    \"\"\"Display sample of text.\"\"\"\n",
    "    if text is None:\n",
    "        print(f\"‚ö†Ô∏è No output available for {method_name}\")\n",
    "        return\n",
    "    \n",
    "    # Extract first N sentences (naive approach)\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "    \n",
    "    for char in text:\n",
    "        current_sentence.append(char)\n",
    "        if char in '.!?':\n",
    "            sentences.append(''.join(current_sentence).strip())\n",
    "            current_sentence = []\n",
    "            if len(sentences) >= num_sentences:\n",
    "                break\n",
    "    \n",
    "    sample = ' '.join(sentences)\n",
    "    \n",
    "    display(Markdown(f\"### {method_name}\"))\n",
    "    display(Markdown(f\"_{sample}_\"))\n",
    "    display(Markdown(f\"**Length:** {len(sample)} characters\"))\n",
    "    print()\n",
    "\n",
    "# Display samples\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE OUTPUTS (First 5 sentences)\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "for method, content in methods.items():\n",
    "    display_sample(method, content[\"output\"], num_sentences=5)\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## Temporal Order Analysis\n",
    "\n",
    "Examine how well each method preserves chronological order by looking at Kendall's Tau scores.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Temporal order comparison\n",
    "tau_data = []\n",
    "\n",
    "for method, content in methods.items():\n",
    "    if content[\"eval\"]:\n",
    "        tau_data.append({\n",
    "            \"Method\": method,\n",
    "            \"Kendall's Tau\": content[\"eval\"].get(\"kendall_tau\", 0)\n",
    "        })\n",
    "\n",
    "df_tau = pd.DataFrame(tau_data)\n",
    "\n",
    "if not df_tau.empty:\n",
    "    display(df_tau.set_index(\"Method\"))\n",
    "    \n",
    "    # Create interpretation\n",
    "    print(\"\\nüìä INTERPRETATION:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for _, row in df_tau.iterrows():\n",
    "        method = row[\"Method\"]\n",
    "        tau = row[\"Kendall's Tau\"]\n",
    "        \n",
    "        if tau >= 0.9:\n",
    "            status = \"‚úÖ Excellent - Near-perfect chronological order\"\n",
    "        elif tau >= 0.7:\n",
    "            status = \"‚úì Good - Strong chronological preservation\"\n",
    "        elif tau >= 0.5:\n",
    "            status = \"‚ö†Ô∏è Moderate - Some chronological structure\"\n",
    "        elif tau >= 0.3:\n",
    "            status = \"‚ö†Ô∏è Weak - Limited chronological order\"\n",
    "        else:\n",
    "            status = \"‚ùå Poor - Random or inverse ordering\"\n",
    "        \n",
    "        print(f\"{method:25} (œÑ={tau:.3f}): {status}\")\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    colors = ['green' if t >= 0.9 else 'orange' if t >= 0.5 else 'red' \n",
    "              for t in df_tau[\"Kendall's Tau\"]]\n",
    "    \n",
    "    ax.barh(df_tau[\"Method\"], df_tau[\"Kendall's Tau\"], color=colors, alpha=0.7)\n",
    "    ax.set_xlabel(\"Kendall's Tau\", fontsize=12)\n",
    "    ax.set_title(\"Temporal Order Preservation\", fontsize=14, fontweight='bold')\n",
    "    ax.set_xlim(0, 1.0)\n",
    "    ax.axvline(x=0.9, color='green', linestyle='--', alpha=0.5, label='Excellent (‚â•0.9)')\n",
    "    ax.axvline(x=0.5, color='orange', linestyle='--', alpha=0.5, label='Moderate (‚â•0.5)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / \"temporal_order_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## Content Coverage Analysis\n",
    "\n",
    "Compare ROUGE-L scores which indicate how well the longest common subsequence is preserved.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# ROUGE-L comparison (indicates content coverage with order)\n",
    "rouge_l_data = []\n",
    "\n",
    "for method, content in methods.items():\n",
    "    if content[\"eval\"]:\n",
    "        rouge_l = content[\"eval\"].get(\"rouge\", {}).get(\"rougeL\", {}).get(\"f1\", 0)\n",
    "        rouge_l_data.append({\n",
    "            \"Method\": method,\n",
    "            \"ROUGE-L F1\": rouge_l\n",
    "        })\n",
    "\n",
    "df_rouge_l = pd.DataFrame(rouge_l_data)\n",
    "\n",
    "if not df_rouge_l.empty:\n",
    "    display(df_rouge_l.set_index(\"Method\"))\n",
    "    \n",
    "    # Interpretation\n",
    "    print(\"\\nüìä INTERPRETATION:\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"ROUGE-L measures the longest common subsequence between generated\")\n",
    "    print(\"text and reference. Higher scores indicate better content coverage\")\n",
    "    print(\"AND better preservation of sequential order.\\n\")\n",
    "    \n",
    "    for _, row in df_rouge_l.iterrows():\n",
    "        method = row[\"Method\"]\n",
    "        score = row[\"ROUGE-L F1\"]\n",
    "        \n",
    "        if score >= 0.9:\n",
    "            status = \"‚úÖ Excellent coverage\"\n",
    "        elif score >= 0.7:\n",
    "            status = \"‚úì Good coverage\"\n",
    "        elif score >= 0.5:\n",
    "            status = \"‚ö†Ô∏è Moderate coverage\"\n",
    "        else:\n",
    "            status = \"‚ùå Limited coverage\"\n",
    "        \n",
    "        print(f\"{method:25} (F1={score:.3f}): {status}\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## Summary and Recommendations\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Generate summary recommendations\n",
    "display(Markdown(\"## üìù ANALYSIS SUMMARY\"))\n",
    "display(Markdown(\"---\"))\n",
    "\n",
    "if not df_metrics.empty:\n",
    "    # Find best method for each metric\n",
    "    best_methods = {}\n",
    "    \n",
    "    for metric in [\"Kendall's Tau\", \"ROUGE-1 F1\", \"ROUGE-2 F1\", \n",
    "                   \"ROUGE-L F1\", \"BERTScore F1\", \"METEOR\"]:\n",
    "        best_idx = df_metrics[metric].idxmax()\n",
    "        best_method = df_metrics.loc[best_idx, \"Method\"]\n",
    "        best_score = df_metrics.loc[best_idx, metric]\n",
    "        best_methods[metric] = (best_method, best_score)\n",
    "    \n",
    "    display(Markdown(\"### Best Performing Method per Metric\"))\n",
    "    \n",
    "    for metric, (method, score) in best_methods.items():\n",
    "        display(Markdown(f\"- **{metric}**: {method} ({score:.3f})\"))\n",
    "    \n",
    "    display(Markdown(\"\\n### Key Findings\"))\n",
    "    \n",
    "    # Temporal order winner\n",
    "    tau_winner = best_methods[\"Kendall's Tau\"]\n",
    "    display(Markdown(f\"1. **Temporal Order**: {tau_winner[0]} achieves the best chronological preservation (œÑ={tau_winner[1]:.3f})\"))\n",
    "    \n",
    "    # Content coverage winner\n",
    "    rouge_l_winner = best_methods[\"ROUGE-L F1\"]\n",
    "    display(Markdown(f\"2. **Content Coverage**: {rouge_l_winner[0]} provides the most comprehensive coverage (F1={rouge_l_winner[1]:.3f})\"))\n",
    "    \n",
    "    # Semantic similarity winner\n",
    "    bert_winner = best_methods[\"BERTScore F1\"]\n",
    "    display(Markdown(f\"3. **Semantic Fidelity**: {bert_winner[0]} maintains the best semantic equivalence (F1={bert_winner[1]:.3f})\"))\n",
    "    \n",
    "    display(Markdown(\"\\n### Recommendations\"))\n",
    "    display(Markdown(\"\"\"\n",
    "- **For chronological accuracy**: Use the method with highest Kendall's Tau\n",
    "- **For comprehensive narratives**: Use the method with highest ROUGE-L\n",
    "- **For fluent text**: Consider abstractive methods (PRIMERA) over extractive (TAEG)\n",
    "- **For exact reproduction**: TAEG's extractive approach provides literal accuracy\n",
    "    \"\"\"))\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No evaluation data available for summary\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## Export Results\n",
    "\n",
    "Save a comprehensive comparison report.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Create comprehensive report\n",
    "report_lines = []\n",
    "report_lines.append(\"=\"*80)\n",
    "report_lines.append(\"COMPREHENSIVE COMPARISON REPORT\")\n",
    "report_lines.append(\"TAEG vs PRIMERA-MDS vs PRIMERA-Consolidation\")\n",
    "report_lines.append(\"=\"*80)\n",
    "report_lines.append(\"\")\n",
    "\n",
    "# Add metrics table\n",
    "if not df_metrics.empty:\n",
    "    report_lines.append(\"METRICS COMPARISON\")\n",
    "    report_lines.append(\"-\"*80)\n",
    "    report_lines.append(df_metrics.to_string())\n",
    "    report_lines.append(\"\")\n",
    "\n",
    "# Add lengths\n",
    "if not df_lengths.empty:\n",
    "    report_lines.append(\"OUTPUT LENGTH COMPARISON\")\n",
    "    report_lines.append(\"-\"*80)\n",
    "    report_lines.append(df_lengths.to_string())\n",
    "    report_lines.append(\"\")\n",
    "\n",
    "# Save report\n",
    "report_file = OUTPUT_DIR / \"analysis_report.txt\"\n",
    "with open(report_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(report_lines))\n",
    "\n",
    "print(f\"‚úÖ Analysis report saved to: {report_file}\")\n",
    "print(f\"üìä Visualizations saved to: {OUTPUT_DIR}\")\n",
    "</VSCode.Cell>\n",
    "````"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
